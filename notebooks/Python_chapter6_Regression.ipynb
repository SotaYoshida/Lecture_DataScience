{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python_chapter6_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SotaYoshida/Lecture_DataScience/blob/2021/notebooks/Python_chapter6_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vysbhio4dRuQ"
      },
      "source": [
        "# 相関・回帰分析\n",
        "\n",
        "*相関関係は因果関係を含意しない (Correlation does not imply causation)*\n",
        "\n",
        "[この章の目的]\n",
        "初歩的な相関分析と回帰分析がPythonで出来るようになる。\n",
        "\n",
        "\n",
        "補足資料: [講義ノート](https://drive.google.com/file/d/1ZKi8DJFSg00xir1IoEQiw3z9vxmejeCv/view)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm1vDuyut69e"
      },
      "source": [
        "今回使用するライブラリをインポートしておきましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd39tZH4t6UZ"
      },
      "source": [
        "from matplotlib import pyplot as plt \n",
        "!pip install japanize-matplotlib \n",
        "import japanize_matplotlib \n",
        "import numpy as np "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k8KOlPCIuaF"
      },
      "source": [
        "## 相関分析 (復習)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm6stZb_IwM1"
      },
      "source": [
        "1年生前期の必修科目[DS入門]でも(多くの学科で)[相関分析]を学習しました。  \n",
        "\n",
        "解析したいデータが２種類だけなら、プログラムを使うありがたみはそれほど感じられないと思いますが  \n",
        "「多くのデータ間の相関関係を系統的に調べたい」  \n",
        "あるいは「複数年度に渡るデータを解析したい」となると、  \n",
        "Excelはデータが大きくなるとすぐに挙動が重くなってしまうため  \n",
        "精神によくありません(私見)ので、Pythonで扱うのがオススメです。\n",
        "\n",
        "以下では、復習を兼ねて簡単な例の相関分析を扱って、  \n",
        "時間に余裕があれば前章のおまけの詳細を扱うことにします。\n",
        "\n",
        "まずは簡単な例から初めましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPWcU6_ylxcu"
      },
      "source": [
        "x= [3.1, 4.3, 6.6, 13.2, 19.1, 20.9, 26.4, 25.1, 21.9, 15.7, 9.6, 3.8]\n",
        "y= [568, 572, 804, 833, 930, 965, 1213, 1120, 835, 540, 451, 502]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK_TO1InnQ2x"
      },
      "source": [
        "上に示したのは、2017年の宇都宮市における月別の平均気温$x$と  \n",
        "世帯ごとのアイスクリーム・シャーベットの平均消費金額$y$です。  \n",
        "散布図にすると↓こんな感じ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0uJSEOQmE47"
      },
      "source": [
        "plt.figure(figsize=(6,6)) \n",
        "plt.title(\"宇都宮市\") \n",
        "plt.xlabel(\"平均気温 (℃)\")\n",
        "plt.ylabel(\"世帯あたりのアイスクリム・シャーベットの消費金額 (円)\")\n",
        "plt.scatter(x,y)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHPfNSoymCxz"
      },
      "source": [
        "「平均気温とアイスの消費には相関がありそう」という直感の通り、  \n",
        "正の相関があることが見て取れます。\n",
        "\n",
        "では\"どれほどの\"相関を持つかを表す量として相関係数を算出してみましょう。  \n",
        "相関係数$r$は以下のように定義されます。\n",
        "$r= \\frac{ \\sum^n_i (x_i-\\bar{x})(y_i-\\bar{y})}{ \\sqrt{\\sum^n_i (x_i-\\bar{x})^2 \\sum^n_i (y_i-\\bar{y})^2} }$  \n",
        "$\\bar{x},\\bar{y}$はそれぞれ$x,y$の平均値で、数式で書くと$\\bar{x} \\equiv \\frac{1}{n} \\sum^n_i x_i $,　$\\bar{y} \\equiv \\frac{1}{n} \\sum^n_i y_i $  \n",
        "という感じです。  \n",
        "ここで$\\equiv$は[定義]を表し、下付き添字$i$は$x$の$i$番目の要素であることを表します。  \n",
        "(つまり$x$をn次元ベクトルとみなしたときの$i$成分が$x_i$とも言えますね)  \n",
        "今考えているデータの場合、$\\sum$の和記号は$i$は1から12までの値を取り  \n",
        "対応する値を足し上げることを意味します。  \n",
        "(\"$i$の和が1から12までを走る\"といったりもする)\n",
        "\n",
        "この$r$の定義では、$r$は必ず-1から1までの値を取り※  \n",
        "1.0(-1.0)に近づくにつれて強い正(負)の相関を持ちます。  \n",
        "(強いというのは曖昧な表現で絶対的な線引がある訳ではありません)\n",
        ">$|r|\\leq1$は、コーシーシュワルツの不等式を用いるか  \n",
        "上の$r$の定義と$n$次元ベクトル同士の内積の定義とを見比べると示すことが出来ます(暇があればやってみましょう)。  \n",
        "\n",
        "では```x``` ,``` y```２つのリストを引数に持ち、この相関係数$r$を返す関数を作成してみましょう。\n",
        "\n",
        "にらめっこするために式を再掲しておきます:\n",
        "$r= \\frac{ \\sum^n_i (x_i-\\bar{x})(y_i-\\bar{y})}{ \\sqrt{\\sum^n_i (x_i-\\bar{x})^2 \\sum^n_i (y_i-\\bar{y})^2} }$  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqr9IFdzoT7A"
      },
      "source": [
        "### ライブラリを一切使わない方法\n",
        "# ※1 xとyの長さが違う場合、長さ0の場合はエラーとし、返り値Noneを返す。\n",
        "# ※2 Pythonではwarning(警告)を返す機能などもあるが、授業では扱わないのでNoneを返す関数とした。\n",
        "\n",
        "def cor_coeff(x,y):\n",
        "    if len(x) != len(y) or len(x)==len(y)==0:\n",
        "        print(\"Error: x&y must satisfy len(x) = len(y) != 0\") # ※1\n",
        "        return None # ※2\n",
        "    n = len(x) \n",
        "    ## 平均を計算\n",
        "    xbar = sum(x)/n; ybar = sum(y)/n \n",
        "\n",
        "    ##分子(numerator)の和を計算 (初期値を0に)\n",
        "    s_n = 0.0 \n",
        "    for i in range(n):\n",
        "        s_n += (x[i]-xbar)*(y[i]-ybar)\n",
        "\n",
        "    ##分母(denominator)の計算 (和を先に計算して積を取り、最後にsquare rootをとる)\n",
        "    s_x = 0.0; s_y = 0.0\n",
        "    for i in range(n):\n",
        "        s_x += (x[i]-xbar)**2 \n",
        "        s_y += (y[i]-ybar)**2\n",
        "    s_d = (s_x * s_y)**0.5\n",
        "    # 一行で書くなら\n",
        "    #s_d = ( sum([(x[i]-xbar)**2 for i in range(n)]) * sum([(y[i]-ybar)**2 for i in range(n)]) )**0.5\n",
        "\n",
        "    return s_n/s_d # 分子/分母の値を返す\n",
        "\n",
        "cor_coeff(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IM8Ebhopmre"
      },
      "source": [
        "という風に、$r$が約0.83で、非常に強い正の相関を示すことがわかりました.\n",
        "\n",
        "numpyライブラリを使うともう少しシンプルに書けるので、それもやっておきましょう."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG4Fie5epxP_"
      },
      "source": [
        "def cor_coeff_np(x,y):\n",
        "    xbar = np.mean(x); ybar=np.mean(y) #np.mean()は整数・実数値が入ったリスト(やnumpy array)の平均を計算\n",
        "    return np.dot(x - xbar,y-ybar) / np.sqrt( np.dot(x-xbar,x-xbar) * np.dot(y-ybar,y-ybar) ) \n",
        "\n",
        "cor_coeff_np(x,y) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "criDMWxYrJUo"
      },
      "source": [
        "とすると、関数自体は数行で書けてしまいました。  \n",
        "さらに$\\bar{x},\\bar{y}$をいちいち定義しないように書き換えれば、関数の中身自体は一行でかけてしまいます。\n",
        "\n",
        "上のコードを少し補足しておくと...分子や分母に現れる  \n",
        "$\\sum^n_i (x_i-\\bar{x})(y_i-\\bar{y})$や$\\sum^n_i (x_i-\\bar{x})^2 $といった項は、  \n",
        "$i$番目の成分に$x_i-\\bar{x}$を持つベクトル$\\tilde{x}$と  \n",
        "$i$番目の成分に$y_i-\\bar{y}$を持つベクトル$\\tilde{y}$を定義しておくと、  \n",
        "$\\tilde{x}\\cdot\\tilde{y}$,　$\\tilde{x}\\cdot\\tilde{x}$,　$\\tilde{y}\\cdot\\tilde{y}$といったように、  \n",
        "ベクトルの内積の形でいずれも表すことができます。\n",
        "\n",
        "numpyにはブロードキャスト機能(Numpyのノートを参照)や  \n",
        "ベクトル積を計算する関数```dot```が備わっているので、  \n",
        "それらを活用することで短く実装することができました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDhJHIZesEMQ"
      },
      "source": [
        "実はnumpyには相関係数を計算する関数```corrcoef()```が予め用意されていて\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ8RiA7ssJUB"
      },
      "source": [
        "print(np.corrcoef(x,y))\n",
        "print(\"r(x,y)=\", np.corrcoef(x,y)[0,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9KiMZZJsNZU"
      },
      "source": [
        "を使えば  \n",
        "[ xとxの相関(=1.0), xとyの相関;  \n",
        "yとxの相関, yとyの相関(=1.0)]  \n",
        "といった2行2列の相関行列を取得することが出来ます。  \n",
        "確かに上の相関行列の[0,1]成分は、  \n",
        "さっき計算した$r$の値と一致しています。\n",
        "\n",
        "「初めからそれを教えろ！」と思うかもしれませんが、  \n",
        "**考えたい量を数式として定義してそれをプログラムに変換し、  \n",
        "値が正しいかどうかを確かめておくのは、式(考え方)とプログラミング  \n",
        "双方の理解を深める上で非常に重要なプロセスです**  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P5hjR0O3AOe"
      },
      "source": [
        "### 相関分析と因果関係\n",
        "\n",
        "前章のファイル操作では、エクセルファイルからデータを読み込んで  \n",
        "系統的に相関分析を行うコードを紹介しました。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NatgqwbX7Fua"
      },
      "source": [
        "以下では、そのうちの一つのグラフを見ながら、  \n",
        "冒頭の*相関関係は因果関係を含意しない (Correlation does not imply causation)*  \n",
        "に関して説明します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o1UP8bT7sXI"
      },
      "source": [
        "下の図は、2017年の家計調査・気候データから作成した散布図で、  \n",
        "千葉市での平均気温と、しめじの消費支出の間の相関を示しています。\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=17DH8RCaU_9DC-fjt_k_On6duL8ixtd31\" width = 50%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM5GOH1q8X5_"
      },
      "source": [
        "\n",
        "生産量と平均気温の間に、強い負の相関が見て取れますが、これはどうしてでしょうか？  \n",
        "「寒い季節には鍋が食べたくなるから」と言われるとふむふむと感じる一方で  \n",
        "「そもそも生産量が冬に多く、市場に出回る量が多いから」と考えることもできます。  \n",
        "したがって、このデータを見ただけでは、しめじが冬によく売れる理由までははっきりとは分かりません。\n",
        "\n",
        "事実、しめじの旬はGoogle検索によると9月下旬から11月初旬とのことで、  \n",
        "最も売れている時期(12月1月)とは少し時期にズレがあり、  \n",
        "購買意欲は必ずしも\"旬\"によって決まっている訳ではなさそうな印象を受けます。\n",
        "\n",
        "気温と特定の野菜の購買意欲の真の関係を知りたければ、  \n",
        "「その野菜はビニールハウスなどの生産設備の向上で年中、安定した味で生産ができる」  \n",
        "「比較的新しい品種で〇〇といえば秋、のような固定観念がない」  \n",
        "「季節ごとの生産量がほぼ同じ」  \n",
        "など、他の条件が揃った状況下で比較しなければ確度の高い議論は難しいでしょう。\n",
        "\n",
        "このように、因果関係を紐解くことは、我々が思うほど容易ではなく、  \n",
        "それ自体が一つの学問体系になっています。  \n",
        "気になる方は、たとえば[因果推論]で調べてみましょう。\n",
        "\n",
        "\n",
        "「我々が見ている相関関係は２次元よりも遥かに高次元の空間での関係  \n",
        "を低次元へ射影した影を見ているに過ぎない」とも言えるでしょう。\n",
        "\n",
        "疑似相関に関するその他の話題は以下を参照してください  \n",
        "* [講義ノート](https://drive.google.com/file/d/1ZKi8DJFSg00xir1IoEQiw3z9vxmejeCv/view)の3.2章  \n",
        "* [疑似相関をまとめたおもしろいサイトはこちら](https://www.tylervigen.com/spurious-correlations)  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqrDeFtadahi"
      },
      "source": [
        "## 回帰分析\n",
        "\n",
        "以下では自分が立てたモデルを表現する関数のことを*モデル関数*、  \n",
        "モデル関数とデータとの齟齬を最小化するようにモデル関数の係数を決定することを**回帰**、  \n",
        "そして回帰に基づく分析を指して**回帰分析**と呼ぶことにします。\n",
        "\n",
        "データとモデル間の齟齬を表現する方法はいくつかありますが、  \n",
        "以下では最もポピュラーな誤差の二乗和を採用することとし、  \n",
        "その最小化を考えていきましょう(最小二乗法とも呼びます)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNqDUVt2U6zV"
      },
      "source": [
        "$D$個の点$\\{x_1,x_2,...,x_D\\}$でのyの値$\\{y_1,y_2,...,y_D\\}$が観測されているとき、  \n",
        "最小二乗法とは、ある決められたモデル関数$f(x)$との齟齬$\\chi^2 = \\sum^D_{i=1} (y_i - f(x_i))^2$を  \n",
        "最小化するように関数$f$の係数を調整することとも言いかえられます。\n",
        "\n",
        "$f$自体をどう決める/設計するかも重要な話題ですが、授業では深入りしません。  \n",
        "たとえば回帰を行う関数として、ニューラルネットワークを採用する立場を採ることも可能です。  \n",
        "参照: [おまけのノートブック: ニューラルネットワークによる回帰](https://colab.research.google.com/github/SotaYoshida/Lecture_DataScience/blob/2021/notebooks/Python_chapter_ArtificialNeuralNetwork.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVBB0OmSdxxo"
      },
      "source": [
        "以下では、$f(x)$として単純な多項式のみを考えることにします。  \n",
        "まず回帰を学ぶために、適当なデータを生成しておきましょう"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IUd5fJ3dZBw"
      },
      "source": [
        "# sample_size: データの数\n",
        "# std: standard deviation (標準偏差σ)\n",
        "def create_toy_data(sample_size, std):\n",
        "    x = np.linspace(0, 1, sample_size)\n",
        "    t = np.sin(2*np.pi*x) + np.random.normal(scale=std, size=x.shape)  \n",
        "    return x, t\n",
        "    \n",
        "np.random.seed(1234) #私と皆さんで結果が変わらないよう乱数のseedを固定, 詳しくは8章で\n",
        "x,y = create_toy_data(10,1.e-1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pThY_bnkd6Ny"
      },
      "source": [
        "これをグラフにしてみると..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnJAT51od7hR"
      },
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\")\n",
        "ax.scatter(x, y, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"Data\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txmm0IZQeh4u"
      },
      "source": [
        "こんな感じ。\n",
        "\n",
        "このデータを、$p$次元の多項式(p=0,1,2,...)でフィッティングすることを考えてみましょう。  \n",
        "$p$次式($p$次元多項式)は、$p+1$個の係数, $a_0$から$a_p$を使って\n",
        "$a_0 + a_1x + a_2x^2\\cdots +a_p x^p $と書くことが出来ます。\n",
        "\n",
        "\n",
        "$p$次元のフィッティングは、実はnumpyにある関数```polyfit()```を利用すれば一瞬で行えます。  \n",
        "\n",
        "(全学部向けの授業なのでとりあえずライブラリを利用することにします。  \n",
        "他にもscikit-learnなどのライブラリもより高度な関数のフィッティングが可能です。  \n",
        "第2回のレポートでは、ごく簡単な場合にもう少し自分の手で愚直にフィッティングをすることを考えてもらう予定です。)\n",
        "\n",
        "\n",
        "> $\\clubsuit$進んだ注:  \n",
        "多項式で回帰を行う場合には、実はパラメータの最適解は\"閉じた形\"で与えられます。  \n",
        "この辺りのことは、おまけのノートブック[ベイズ線形回帰](https://colab.research.google.com/github/SotaYoshida/Lecture_DataScience/blob/2021/notebooks/Python_chapter_Bayesian_linear_regression.ipynb)で詳しく書いています。  \n",
        "なお\"閉じた形\"というのは、数学や物理をやっていると出てくる表現で、  \n",
        "答えが具体的な形で書き下せる、程度の意味です。  \n",
        "たとえば 行列$A$、ベクトル$\\vec{x},\\vec{y}$,スカラー$\\lambda$について方程式$A\\vec{x}=\\lambda \\vec{y}$が成り立つとき、  \n",
        "$A$の逆行列をどうやって求めるか(数値的にやるのか解析的に求めるのか)はさておき、  \n",
        "$\\vec{x} = \\lambda A^{-1}\\vec{y}$と書き直せるので  \n",
        "「$\\vec{x}$は閉じた形で与えられる」と言ったりします。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyOJpVE7xw5p"
      },
      "source": [
        "### polyfit/poly1d関数\n",
        "\n",
        "たとえば今のデータを３次式でフィットしたければ、以下のようにします"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDkIX4Y2jFDc"
      },
      "source": [
        "## 多項式をplotするためのxの値を準備(グラフをなめらかにするために、0から1までの間の500点を等間隔に取る)\n",
        "xp = np.linspace(0, 1, 500) \n",
        "\n",
        "p=3 #多項式の次元を決める. 今は3次式.\n",
        "coeff = np.polyfit(x, y, p) \n",
        "yp = np.poly1d( coeff )(xp)\n",
        "\n",
        "print(\"係数\",coeff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAkx9OThjvX_"
      },
      "source": [
        "```np.polyfit(x, y, p)```では、データのx,yの値と多項式の次元pを引数として与え、  \n",
        "$p$次の多項式でデータ$(x,y)$をfitしなさい  \n",
        "(つまり、$p$次までの係数を関数がデータと整合するように\"最適化\"しなさい)  \n",
        "という指令を与えています.\n",
        "\n",
        "```np.poly1d( np.polyfit(x, y, p) )(xp)```では、  \n",
        "fitしたp次元の係数をもつ多項式に```xp```(今は500点)を代入して、対応する```y```の値を返します。  \n",
        "上のコードはこの返り値をypという変数に格納しています。\n",
        "\n",
        "最後に、調整(最適化)された3次式の係数を表示してみました。  \n",
        "ちなみに、表示される係数は次数が高いところから$a_3,a_2,a_1,a_0$です(ややこしい...)。\n",
        "\n",
        "グラフを描いてみるとこんな感じ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E-yeiTVB2ep"
      },
      "source": [
        "#お絵かき\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\")\n",
        "ax.scatter(x, y, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"Data\")\n",
        "ax.plot(xp, yp,label=\"p=3\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXSQcr3HYbQa"
      },
      "source": [
        "\n",
        "さて、$p$次の多項式は$p-1$次の多項式を特別な場合として含むため、  \n",
        "一般に$p$(多項式の次元)を増やせば、より複雑な関数を表現することができます。  \n",
        "(2次式は3次式の$a_3=0$の場合ですよね？)\n",
        "\n",
        "$p$を複数変えながら比較した図を作ってみましょう。  \n",
        "方法は$p$に関するループを回すだけです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GTTZ_cjhiV1"
      },
      "source": [
        "ps = [0,1,3,6,9]\n",
        "ys = []\n",
        "xp = np.linspace(0, 1, 500) ## 多項式をplotするためのxの値を準備\n",
        "for p in ps:\n",
        "    ys += [np.poly1d(np.polyfit(x, y, p))(xp)]\n",
        "ytrue = np.sin(2*np.pi*xp) \n",
        "\n",
        "fig = plt.figure(figsize=(12,5))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\")\n",
        "ax.scatter(x, y, facecolor=\"none\", edgecolor=\"b\", s=80, label=\"Data\")\n",
        "for i in range(len(ps)):\n",
        "    ax.plot(xp, ys[i],label=\"p=\"+str(ps[i]),alpha=0.8)\n",
        "ax.plot(xp,ytrue,linestyle=\"dotted\", label=\"True\",color=\"k\")\n",
        "ax.legend(loc=\"upper right\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlSJwJXif_ZK"
      },
      "source": [
        "> 注: 今の場合、コードをよく見るとデータはsin関数に適当なノイズを足したものだということが分かります。  \n",
        "解析の手法を学ぶ際には、このように答えを知っている状態からはじめて、手法がうまくデータを説明しているかどうかを検証したりします。一見ズルっぽいですが、重要なプロセスです。  \n",
        "\n",
        "現実のデータ解析の状況では、背後にある\"真の関数\"が分かっていることは非常に稀ですし、  \n",
        "「興味のあるデータが、人間がよく知っている単純な式(有限次元の多項式や指数関数)で  \n",
        "完全に表現できる道理はない」ということも抑えておくべき重要な点です.  \n",
        "真の関数というのは一般に[神のみぞ知る]で、  \n",
        "人間ができることは出来るだけ尤もらしい関数を見つけ、  \n",
        "その背後にあるメカニズム(の主要部分)を解明することです.  \n",
        "\n",
        "一般に、関数をどんどん複雑なものにしていくにつれて、表現できるデータの幅は大きく拡がります。  \n",
        "その一方で、用意した関数がデータに過度に適合するあまり、  \n",
        "**未知の点での値の予測精度(汎化性能)が著しく損なわれている危険性があります**。  \n",
        "このことを予言能力がない(データに過適合している) と言ったりします。  \n",
        "データの背後にあるメカニズムが何かを考えたり理論的な解析をして初めて、  \n",
        "回帰に用いる関数の妥当性が検証できるという点に注意しましょう。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbsTjw4FydNh"
      },
      "source": [
        "### $\\clubsuit$ モデルの複雑さとモデル選択"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eo-o3kjyhaQ"
      },
      "source": [
        "上の多項式回帰では、たとえば9次式はデータをピッタリと再現している一方で  \n",
        "真の関数(sin関数)の振る舞いよりもむしろ、測定誤差のようなものにまで過適合してしまっています。\n",
        "\n",
        "\n",
        "ここで過適合を防ぐためにデータとの整合性(二乗誤差)だけでなく  \n",
        "**モデルの複雑さ**も定量化して勘定することを考える。  \n",
        "\n",
        "ここではこのモデルの複雑さ$C$として多項式の係数の絶対値の2乗和:  \n",
        "$C= \\sum_i |a_i|^2$を採用することにしよう。  \n",
        "\n",
        "\n",
        "さらに、\"モデルを選択するための基準$L$\"を  \n",
        "$L = $(二乗誤差) + $\\lambda$  log10(モデルの複雑さ$C$)で定量化し  \n",
        "この$L$が最小になる多項式を採用することにしよう。  \n",
        "(この選択はあくまで例であることに注意)\n",
        "\n",
        "各次数での多項式のモデルの複雑さ$C$と二乗誤差、そしてモデル選択基準量$L$を表示してみると...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn3Bv6wkzMgT"
      },
      "source": [
        "def complexity(r):\n",
        "    return np.sqrt(np.dot(r,r))\n",
        "def my_criteria(comp,err,lam=1.0): #lambda=1.0\n",
        "    return err + lam * np.log10(comp)\n",
        "\n",
        "for p in ps:\n",
        "    coeff = np.polyfit(x, y, p)\n",
        "    diff = np.poly1d(np.polyfit(x, y, p))(x) - y\n",
        "    chi2 = np.dot(diff,diff)\n",
        "    comp = complexity(coeff)\n",
        "    print(\"p\",p, \"モデルの複雑さ(log10)→\", np.log10(comp),\n",
        "          \"二乗誤差\", chi2, \"モデル選択基準量\", my_criteria(comp,chi2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoNEkcu3049k"
      },
      "source": [
        "9次式は、データをよく説明する一方で、非常に複雑なモデルになっている。  \n",
        "\n",
        "上記のモデル選択基準量$L$は$p=3$で最小となるため  \n",
        "この$L$の定義のもとでは3次式が選ばれることになる。\n",
        "\n",
        "このように実際のデータ分析や機械学習などのモデル選択では、  \n",
        "データの説明(たとえば二乗誤差)と複雑さのトレードオフで  \n",
        "できるだけ過適合を避けるようなモデルを選択することを目指す。\n",
        "\n",
        "上の$L$の定義中の$\\lambda$の大きさを変えることは    \n",
        "データとの整合性を高める/モデルの複雑さを抑える  \n",
        "のどちらを重視するかの\"度合い\"を決めることに相当する。  \n",
        "($\\lambda$を適当に変えてみよう)\n",
        "\n",
        "参考→正則化でググってみよう.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Qe_dbjWN-j"
      },
      "source": [
        "### (余談1) 100メートル走のタイム\n",
        "\n",
        "上記のことに関連して、こういう例を考えてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4n_wIoM14c5"
      },
      "source": [
        "y = [ 10.06, 10.03,10.02, 9.95,9.93, 9.92,9.9, 9.86,9.85, 9.84, 9.79, 9.78, 9.77, 9.74,9.72,9.69,9.58 ]\n",
        "x = [1964, 1968,1968,1968,1983,1988,1991,1991,1994,1996,1999,2002,2005,2007,2008,2008,2009 ]\n",
        "\n",
        "fig = plt.figure(figsize=(12,3))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_xlabel(\"year\"); ax.set_ylabel(\"Mens 100m\")\n",
        "ax.scatter(x,y,marker=\"o\",color=\"red\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcJZySlf3Uq7"
      },
      "source": [
        "図にしたのは、男子100mの世界記録の推移です。\n",
        "\n",
        "このデータに対して「$p=3$の多項式でフィットして予測する」という立場をとってみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCUOlGTa3Q7B"
      },
      "source": [
        "xp = np.arange(2020,2101,1)\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_xlabel(\"year\"); ax.set_ylabel(\"Mens 100m\")\n",
        "ax.set_xlim(1960,2100)\n",
        "ax.set_ylim(0,12)\n",
        "for p in [3]:\n",
        "    yp = np.poly1d(np.polyfit(x, y, p))(xp)\n",
        "    ax.plot(xp,yp,marker=\"x\",label=\"p=\"+str(p))\n",
        "ax.scatter(x,y,marker=\"x\",color=\"red\")    \n",
        "ax.legend(loc=\"upper right\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lebt_UTs4PAe"
      },
      "source": [
        "当然おかしな予測だと気がつくはずです。  \n",
        "なぜなら、2080年代には100m走のタイムがゼロになってしまうからです。\n",
        "\n",
        "今の場合、我々はこのデータが100走の世界記録のタイムの推移であること、つまり\n",
        "* 非増加関数であること\n",
        "* 必ず正の値であること\n",
        "\n",
        "など、データが持つべき性質を予め知っているので、  \n",
        "「このデータに対して単純な多項式回帰を当てはめるのはおかしい」  \n",
        "と気がつくことが出来ます。  \n",
        "\n",
        "**でも、他のデータではどうでしょうか？**\n",
        "\n",
        "データを分析するためには、データの値だけをみて闇雲に分析するだけではダメで、  \n",
        "データの背景やドメイン知識が不可欠である、という好例です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL4J1CvJ9ciq"
      },
      "source": [
        "### (余談2) 新型コロナウイルス感染症の陽性者数の推移に関して\n",
        "回帰分析に関連して、この話題も取り上げておきましょう。  \n",
        "\n",
        "我々が現実世界で観測することのできる種々の[値]というのは、  \n",
        "何らかの関数$f(x)$の、ある$x$での(実現)値と言えるかと思います。  \n",
        "テレビで見るコロナウイルスの感染者数の推移も日付に対する関数になっていたりします。\n",
        "\n",
        "ただし一般に物事の背景にある\"真の関数\"というのは多次元の変数に対する複雑な関数になっているはずで、  \n",
        "コロナウイルスの感染者数の推移も単なる時間に対する１変数の関数であるはずなどがありません。  \n",
        "\n",
        "日付に対して陽性者数の推移をプロットして「このままだと指数関数的に増加する」  \n",
        "という予想を立てることは簡単ですが、一般に物事は多様な要素が絡み合っています。  \n",
        "たとえば検査数や我々の外出自粛や国・都道府県ごとの取り組み・政策,  \n",
        "ウイルスの変異,その他様々な要素に左右されるはずです。\n",
        "\n",
        "我々人間がグラフにして理解できるのはたかだか３次元(３つの変数がある状況)までです。  \n",
        "言い換えれば、人間は物事を理解するときに本来D次元(D>>3, Dは３よりずっと大きい)  \n",
        "の変数で定義される関数を、3次元以下に射影した「影」をみて理解しようとする生き物だということは、  \n",
        "意識しておくべきでしょう。\n",
        "\n",
        "\n",
        "一度目の感染ピークが訪れて緊急事態宣言が出され、  \n",
        "報道が加熱していた頃には、「ぼく(わたし)が考えた最強のモデル」  \n",
        "を使って感染者数を予測して危険を煽ったり、あるいは逆に  \n",
        "「過度に心配する必要がない」などと主張したりする専門家・非専門家が数多くいました。  \n",
        "また事態が収束したあとに「私のモデルはこんなに正しかった」という人も現れることでしょう。  \n",
        "ですが、それは極めて高い蓋然性で偶然です。  \n",
        "無限の数の関数を考えれば、データに適合するものが存在してもおかしくはありません。  \n",
        "何にでも言えることですが、モデルを立てて終わり、ではなく検証する姿勢が重要です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPK_KIGcyuod"
      },
      "source": [
        "# LICENSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q943wB7Z4DYK"
      },
      "source": [
        "\n",
        "Copyright (C) 2021 Sota Yoshida\n",
        "\n",
        "[ライセンス:クリエイティブ・コモンズ 4.0 表示 (CC-BY 4.0)](https://creativecommons.org/licenses/by/4.0/deed.ja)"
      ]
    }
  ]
}